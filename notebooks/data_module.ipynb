{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import logging\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch import (\n",
    "    cli_lightning_logo,\n",
    "    LightningDataModule,\n",
    "    LightningModule,\n",
    "    Trainer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformTypes = Optional[Union[A.Compose, T.Compose]]\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.003\n",
    "IMAGE_SIZE = 224\n",
    "TRAIN_DATA_PATH: str = path.join(\"..\", \"data\", \"train\")\n",
    "VALIDATION_DATA_PATH: str = path.join(\"..\", \"data\", \"val\")\n",
    "TEST_DATA_PATH: str = path.join(\"..\", \"data\", \"test\")\n",
    "\n",
    "\n",
    "TRAIN_TRANSFORM_IMG = T.Compose(\n",
    "    [\n",
    "        T.Resize(IMAGE_SIZE),\n",
    "        T.CenterCrop(IMAGE_SIZE),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "TEST_TRANSFORM_IMG = T.Compose(\n",
    "    [\n",
    "        T.Resize(IMAGE_SIZE),\n",
    "        T.CenterCrop(IMAGE_SIZE),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "# TRAIN_TRANSFORM_IMG = A.Compose(\n",
    "#     [\n",
    "#         A.augmentations.crops.transforms.RandomResizedCrop(\n",
    "#             height=IMAGE_SIZE, width=IMAGE_SIZE, scale=[0.9, 1], ratio=[1, 1]\n",
    "#         ),\n",
    "#         A.augmentations.transforms.Normalize(\n",
    "#             mean=[0.4913997551666284, 0.48215855929893703, 0.4465309133731618],\n",
    "#             std=[0.24703225141799082, 0.24348516474564, 0.26158783926049628],\n",
    "#         ),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )\n",
    "# TEST_TRANSFORM_IMG = A.Compose(\n",
    "#     [\n",
    "#         A.augmentations.geometric.resize.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "#         A.augmentations.transforms.Normalize(\n",
    "#             mean=[0.4913997551666284, 0.48215855929893703, 0.4465309133731618],\n",
    "#             std=[0.24703225141799082, 0.24348516474564, 0.26158783926049628],\n",
    "#         ),\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def create_model(num_classes: int = 2):\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    model = resnet50(weights=weights)\n",
    "    set_parameter_requires_grad(model, feature_extracting=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, num_classes: int = 2, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 1 * 1, 120),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(84, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(LightningModule):\n",
    "    \"\"\"\n",
    "    >>> LitClassifier(Backbone())  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    LitClassifier(\n",
    "      (backbone): ...\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, backbone: Optional[Backbone] = None, learning_rate: float = 0.0001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"backbone\"])\n",
    "        if backbone is None:\n",
    "            backbone = Backbone()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        # use forward for inference/predictions\n",
    "        embedding = self.backbone(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"valid_loss\", loss, on_step=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        x, y = batch\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams available because we called self.save_hyperparameters()\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.train_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=TRAIN_DATA_PATH, transform=TRAIN_TRANSFORM_IMG\n",
    "        )\n",
    "        self.validation_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=VALIDATION_DATA_PATH, transform=TEST_TRANSFORM_IMG\n",
    "        )\n",
    "        self.test_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=TEST_DATA_PATH, transform=TEST_TRANSFORM_IMG\n",
    "        )\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validation_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type   | Params\n",
      "------------------------------------\n",
      "0 | backbone | ResNet | 23.5 M\n",
      "------------------------------------\n",
      "4.1 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.049    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/xnn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/xnn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 163/163 [01:00<00:00,  2.68it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 163/163 [01:01<00:00,  2.67it/s, v_num=2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/furyhawk/Explainable-Neural-Networks/notebooks/lightning_logs/version_2/checkpoints/epoch=1-step=326.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/furyhawk/Explainable-Neural-Networks/notebooks/lightning_logs/version_2/checkpoints/epoch=1-step=326.ckpt\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/xnn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 20/20 [00:06<00:00,  3.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Runningstage.testing    </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4551825523376465     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4551825523376465    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/furyhawk/Explainable-Neural-Networks/notebooks/lightning_logs/version_2/checkpoints/epoch=1-step=326.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/furyhawk/Explainable-Neural-Networks/notebooks/lightning_logs/version_2/checkpoints/epoch=1-step=326.ckpt\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/xnn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 20/20 [00:08<00:00,  2.25it/s]\n",
      "tensor([[ 0.0389, -0.3110],\n",
      "        [-0.3541,  0.2206],\n",
      "        [ 0.4866, -0.4310],\n",
      "        [-0.4913,  0.5149],\n",
      "        [ 0.2809, -0.2115],\n",
      "        [ 0.2992, -0.5055],\n",
      "        [-0.1557,  0.2244],\n",
      "        [ 0.0430, -0.1161],\n",
      "        [ 0.1052, -0.1405],\n",
      "        [ 0.1736, -0.1959],\n",
      "        [ 0.1231, -0.0484],\n",
      "        [-0.4312,  0.4116],\n",
      "        [ 0.2958, -0.2707],\n",
      "        [-0.0429,  0.1251],\n",
      "        [-0.1399,  0.2113],\n",
      "        [-0.0820,  0.2951],\n",
      "        [-1.0355,  1.1248],\n",
      "        [ 0.0376,  0.1329],\n",
      "        [ 0.7288, -0.5405],\n",
      "        [ 0.0058,  0.2159],\n",
      "        [ 0.2130, -0.2014],\n",
      "        [-0.1570,  0.0375],\n",
      "        [ 0.3130, -0.2778],\n",
      "        [ 0.0967, -0.3230],\n",
      "        [ 0.1634, -0.0106],\n",
      "        [ 0.1893, -0.1183],\n",
      "        [ 0.1806, -0.0971],\n",
      "        [ 0.1242, -0.0677],\n",
      "        [-0.0372, -0.0669],\n",
      "        [-0.3038,  0.3310],\n",
      "        [ 0.3326, -0.2008],\n",
      "        [-0.0511, -0.0314]])\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=EPOCHS\n",
    ")  # , callbacks=[EarlyStopping(monitor=\"valid_loss\", mode=\"min\")]\n",
    "model = ImageClassifier(backbone=create_model())\n",
    "data_module = ImageClassificationDataModule()\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "trainer.test(ckpt_path=\"best\", datamodule=data_module)\n",
    "predictions = trainer.predict(ckpt_path=\"best\", datamodule=data_module)\n",
    "print(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (backbone): Backbone(\n",
       "    (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=120, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=120, out_features=84, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=84, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
